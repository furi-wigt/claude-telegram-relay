/**
 * Document Ingestion Script
 *
 * Reads insurance policy documents (PDF, Markdown, TXT) from a local folder,
 * splits them into overlapping chunks, and inserts each chunk into the
 * `documents` Supabase table. Embeddings are auto-generated by the existing
 * `embed` Edge Function via database webhook.
 *
 * Usage:
 *   bun run scripts/ingest-documents.ts --path ~/Documents/insurance --title "AIA Life Policy"
 *   bun run scripts/ingest-documents.ts --file ./policy.md --title "NTUC Income PA"
 *   bun run scripts/ingest-documents.ts --path ~/Documents/insurance  # auto-title from filename
 *   bun run scripts/ingest-documents.ts --list    # list ingested documents
 *   bun run scripts/ingest-documents.ts --delete "AIA Life Policy"  # remove all chunks for title
 *
 * Options:
 *   --path <dir>    Ingest all .txt / .md / .pdf files in directory
 *   --file <path>   Ingest a single file
 *   --title <name>  Document title (default: filename without extension)
 *   --chunk <n>     Chunk size in chars (default: 1800)
 *   --overlap <n>   Overlap between chunks in chars (default: 200)
 *   --dry-run       Preview chunks without inserting
 *   --list          List all ingested documents
 *   --delete <title> Delete all chunks for a document title
 */

import { createClient } from "@supabase/supabase-js";
import { readFileSync, readdirSync, statSync } from "fs";
import { join, extname, basename } from "path";

// ─── Config ─────────────────────────────────────────────────────────────────

const SUPABASE_URL = process.env.SUPABASE_URL;
const SUPABASE_ANON_KEY = process.env.SUPABASE_ANON_KEY;

if (!SUPABASE_URL || !SUPABASE_ANON_KEY) {
  console.error("ERROR: SUPABASE_URL and SUPABASE_ANON_KEY must be set in environment.");
  console.error("Load your .env first: source .env && bun run scripts/ingest-documents.ts ...");
  process.exit(1);
}

const supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY);

// ─── CLI Args ────────────────────────────────────────────────────────────────

const args = process.argv.slice(2);

function getArg(flag: string): string | null {
  const idx = args.indexOf(flag);
  return idx !== -1 && args[idx + 1] ? args[idx + 1] : null;
}

const DRY_RUN  = args.includes("--dry-run");
const DO_LIST  = args.includes("--list");
const DO_DELETE = args.indexOf("--delete") !== -1 ? args[args.indexOf("--delete") + 1] : null;
const FILE_PATH = getArg("--file");
const DIR_PATH  = getArg("--path");
const TITLE_ARG = getArg("--title");
const CHUNK_SIZE   = parseInt(getArg("--chunk")   ?? "1800", 10);
const CHUNK_OVERLAP = parseInt(getArg("--overlap") ?? "200",  10);

// ─── Chunking ────────────────────────────────────────────────────────────────

/**
 * Split text into overlapping chunks.
 * Tries to break on paragraph boundaries (double newline) first,
 * then falls back to hard split at CHUNK_SIZE.
 */
function chunkText(text: string, chunkSize = CHUNK_SIZE, overlap = CHUNK_OVERLAP): string[] {
  const paragraphs = text.split(/\n{2,}/);
  const chunks: string[] = [];
  let current = "";

  for (const para of paragraphs) {
    const trimmed = para.trim();
    if (!trimmed) continue;

    if (current.length + trimmed.length + 2 > chunkSize) {
      if (current) {
        chunks.push(current.trim());
        // Keep last `overlap` chars as context for next chunk
        current = current.length > overlap ? current.slice(-overlap) + "\n\n" + trimmed : trimmed;
      } else {
        // Single paragraph exceeds chunk size — hard split
        for (let i = 0; i < trimmed.length; i += chunkSize - overlap) {
          chunks.push(trimmed.slice(i, i + chunkSize));
        }
        current = "";
      }
    } else {
      current = current ? current + "\n\n" + trimmed : trimmed;
    }
  }

  if (current.trim()) {
    chunks.push(current.trim());
  }

  return chunks.filter((c) => c.length > 50); // skip near-empty chunks
}

// ─── PDF Text Extraction ─────────────────────────────────────────────────────

async function extractPdfText(filePath: string): Promise<string> {
  // Try pdf-parse if available, otherwise instruct user
  try {
    const { default: pdfParse } = await import("pdf-parse");
    const buffer = readFileSync(filePath);
    const data = await pdfParse(buffer);
    return data.text;
  } catch {
    console.warn(`pdf-parse not available. Install with: bun add pdf-parse`);
    console.warn(`Alternatively, convert the PDF to .txt or .md first.`);
    console.warn(`Skipping: ${filePath}`);
    return "";
  }
}

// ─── File Reading ─────────────────────────────────────────────────────────────

async function readFileText(filePath: string): Promise<string> {
  const ext = extname(filePath).toLowerCase();
  if (ext === ".pdf") {
    return extractPdfText(filePath);
  }
  return readFileSync(filePath, "utf-8");
}

// ─── Ingestion ────────────────────────────────────────────────────────────────

async function ingestFile(filePath: string, title: string): Promise<void> {
  console.log(`\nIngesting: ${filePath}`);
  console.log(`  Title: "${title}"`);

  const text = await readFileText(filePath);
  if (!text.trim()) {
    console.warn("  No text extracted, skipping.");
    return;
  }

  const source = basename(filePath);
  const chunks = chunkText(text);
  console.log(`  Chunks: ${chunks.length} (size ≤${CHUNK_SIZE} chars, overlap ${CHUNK_OVERLAP})`);

  if (DRY_RUN) {
    for (let i = 0; i < chunks.length; i++) {
      console.log(`\n  [Chunk ${i + 1}/${chunks.length}] (${chunks[i].length} chars)`);
      console.log("  " + chunks[i].substring(0, 200).replace(/\n/g, "\n  ") + (chunks[i].length > 200 ? "…" : ""));
    }
    console.log("\n  DRY RUN — no rows inserted.");
    return;
  }

  // Delete existing chunks for this source file (re-ingestion support)
  const { error: delErr } = await supabase
    .from("documents")
    .delete()
    .eq("title", title)
    .eq("source", source);

  if (delErr) {
    console.warn(`  Warning: could not delete existing chunks: ${delErr.message}`);
  }

  // Insert all chunks
  const rows = chunks.map((content, chunk_index) => ({
    title,
    source,
    chunk_index,
    content,
    metadata: { chars: content.length },
  }));

  const { error } = await supabase.from("documents").insert(rows);
  if (error) {
    console.error(`  ERROR inserting chunks: ${error.message}`);
    return;
  }

  console.log(`  ✓ Inserted ${chunks.length} chunks (embeddings auto-generating via webhook)`);
}

// ─── Commands ─────────────────────────────────────────────────────────────────

async function listDocuments(): Promise<void> {
  const { data, error } = await supabase
    .from("documents")
    .select("title, source, chunk_index, created_at")
    .order("created_at", { ascending: false });

  if (error) {
    console.error("Error fetching documents:", error.message);
    return;
  }

  if (!data?.length) {
    console.log("No documents ingested yet.");
    return;
  }

  // Group by title
  const byTitle = new Map<string, { sources: Set<string>; chunks: number; latest: string }>();
  for (const row of data) {
    if (!byTitle.has(row.title)) {
      byTitle.set(row.title, { sources: new Set(), chunks: 0, latest: row.created_at });
    }
    const entry = byTitle.get(row.title)!;
    entry.sources.add(row.source);
    entry.chunks++;
  }

  console.log("\nIngested Documents\n" + "─".repeat(50));
  for (const [title, info] of byTitle.entries()) {
    console.log(`\n  "${title}"`);
    console.log(`    Sources : ${[...info.sources].join(", ")}`);
    console.log(`    Chunks  : ${info.chunks}`);
    console.log(`    Ingested: ${new Date(info.latest).toLocaleDateString("en-SG")}`);
  }
}

async function deleteDocument(title: string): Promise<void> {
  const { count, error } = await supabase
    .from("documents")
    .delete({ count: "exact" })
    .eq("title", title);

  if (error) {
    console.error(`Error deleting "${title}": ${error.message}`);
    return;
  }

  console.log(`Deleted ${count} chunks for "${title}".`);
}

// ─── Main ─────────────────────────────────────────────────────────────────────

async function main(): Promise<void> {
  if (DO_LIST) {
    await listDocuments();
    return;
  }

  if (DO_DELETE) {
    await deleteDocument(DO_DELETE);
    return;
  }

  if (!FILE_PATH && !DIR_PATH) {
    console.error(
      "Usage:\n" +
      "  bun run scripts/ingest-documents.ts --file <path> [--title <name>]\n" +
      "  bun run scripts/ingest-documents.ts --path <dir> [--title <name>]\n" +
      "  bun run scripts/ingest-documents.ts --list\n" +
      "  bun run scripts/ingest-documents.ts --delete <title>\n"
    );
    process.exit(1);
  }

  if (FILE_PATH) {
    const title = TITLE_ARG ?? basename(FILE_PATH, extname(FILE_PATH));
    await ingestFile(FILE_PATH, title);
    return;
  }

  if (DIR_PATH) {
    const expandedPath = DIR_PATH.replace(/^~/, process.env.HOME ?? "~");
    const files = readdirSync(expandedPath).filter((f) => {
      const ext = extname(f).toLowerCase();
      return [".txt", ".md", ".pdf"].includes(ext);
    });

    if (!files.length) {
      console.log(`No .txt, .md, or .pdf files found in: ${expandedPath}`);
      return;
    }

    console.log(`Found ${files.length} file(s) in ${expandedPath}`);

    for (const file of files) {
      const filePath = join(expandedPath, file);
      if (statSync(filePath).isFile()) {
        const title = TITLE_ARG ?? basename(file, extname(file));
        await ingestFile(filePath, title);
      }
    }
  }
}

main().catch((err) => {
  console.error("Fatal:", err);
  process.exit(1);
});
